# üéµ Data Engineering: ETL Pipeline with Spark

## Introduction

Hey there! üëã

Welcome to my project on building an **ETL pipeline with Spark**. I'm working with a music streaming startup called Sparkify, which has grown their user base and song database significantly. They want to move their data warehouse to a data lake. üåä

Sparkify's data resides in S3, with directories of JSON logs on user activity and JSON metadata on songs. As their data engineer, I've been tasked with building an ETL pipeline that:

- **Extracts** data from S3
- **Processes** it using Spark
- **Loads** it back into S3 as a set of dimensional tables

This will enable Sparkify's analytics team to find insights into what songs users are listening to. üé∂

## Summary of the Project

Sparkify aims to analyze the data they've collected on songs and user activity. Their analytics team is particularly interested in understanding user listening patterns. Currently, the data is stored in JSON logs and metadata files on S3, making it difficult to query.

My role involves:

1. Creating an **ETL pipeline** to process this data
2. Designing a **relational database schema** for analysis
3. Ensuring the data can be queried easily for insights

## How to Run the Python Scripts

To start the ETL pipeline, run the following command:

```bash
python3 etl.py
```

## Database Schema Design and ETL Pipeline

To enable Sparkify to analyze their data, I created a **Relational Database Schema** filled via the ETL pipeline. This schema follows a **star schema** design, which allows viewing user behavior across several dimensions. 

- **Fact Table**: `songplays`
- **Dimension Tables**: `users`, `songs`, `artists`, `time`

This setup helps Sparkify relate and analyze data about users, songs, artists, and time efficiently. ‚è∞

## Dataset Used

The data is queried from S3 buckets hosted on AWS:

1. **Song Data**: A subset of real data from the Million Song Dataset. Each JSON file contains metadata about a song and its artist. Files are partitioned by the first three letters of each song's track ID.
2. **Log Data**: JSON log files generated by an event simulator, based on the songs in the dataset. These logs simulate user activity on the music streaming app.

## Use Cases

Here are some potential use cases for this ETL pipeline:

1. **User Behavior Analysis**: Understand what songs users are listening to most frequently.
2. **Artist Popularity Tracking**: Analyze which artists are gaining popularity over time.
3. **Session Analysis**: Track user sessions to determine peak usage times and user engagement.
4. **Recommendation Systems**: Use listening patterns to recommend new songs and artists to users.
5. **Marketing Insights**: Identify trends and patterns for targeted marketing campaigns.

